# ğŸ¥ Chunk, Transcribe, and Analyze Classroom Video

This repository contains a collection of basic scripts I've written to experiment with different approaches to analyzing classroom video. The goal is to automate the process of evaluating teaching sessions through a combination of video chunking, transcription, and performance analysis.

The scripts in this repo include attempts using:

* **OpenAI Whisper** for transcription and evaluation
* **Google Cloud Video Intelligence API** for activity recognition and segmentation passed through to **GPT 4o** for scoring
* **DemucsÂ + Whisper (GPU) for teacherâ€‘voice isolation** â† current most accurate, first real usable output for this project. 

Each script reflects a different approach. Below are descriptions of individual attempts, observations, and how to run them.

---

## `whisperaudio_chunk_transcribe_analyze.py`

### ğŸš€ What It Does

* Uses Whisper's `base` model to transcribe audio from 1-minute video chunks
* Sends the combined transcript to GPT-4o for evaluation on:

  * Clarity and articulation
  * Engagement and questioning style
  * Classroom management
  * Use of feedback and encouragement
  * Instructional structure

### ğŸ“ˆ Early Observations

* **Pros:**

  * Whisper is relatively fast and accurate for clean audio
  * GPT-4o generates thoughtful evaluations when given full transcripts

* **Cons:**

  * Whisper struggles with noisy environments or crosstalk
  * GPT evaluation depends heavily on transcription quality
  * Lacks speaker separation and timestamped analysis (next goal)

### ğŸ“¦ Requirements

* Python 3.9+
* [ffmpeg](https://ffmpeg.org/download.html)
* [OpenAI Python SDK](https://github.com/openai/openai-python)
* [Whisper](https://github.com/openai/whisper)

Install dependencies:

```bash
pip install openai whisper
```

Make sure `ffmpeg` is installed and in your system path.

### ğŸ”‘ Setup

Set your OpenAI API key as an environment variable:

```bash
export OPENAI_API_KEY="sk-..."
```

### â–¶ï¸ Usage

```bash
python transcribe_and_evaluate.py path/to/video.mp4
```

Output:

* Full transcript printed to console
* Teacher performance analysis printed after transcription

---

## `GCPVideoAI_GPT4o_Pipe`

### ğŸš€ What It Does

* Uses **Google Cloud Video Intelligence API** to generate activity labels and speech transcripts from classroom video
* Passes both labels and transcripts **into GPT-4o** to analyze teacher performance within a structured timeline

### ğŸ“ˆ Early Observations

* **What worked:**

  * Nothing really - the pipeline ran succesfully but produced no output 

* **What didnâ€™t work:**

  * Poor video quality (shot on a phone from many angles) degraded both transcription and video labeling quality
  * Inconsistent framing and audio resulted in a weak input timeline for GPT-4o evaluation

### ğŸ“¦ Requirements

* Python 3.9+
* [Google Cloud SDK](https://cloud.google.com/sdk/docs/install)
* `google-cloud-videointelligence` Python package
* [OpenAI Python SDK](https://github.com/openai/openai-python)

Install dependencies:

```bash
pip install google-cloud-videointelligence openai
```

### ğŸ”‘ Setup

1. Enable the **Video Intelligence API** in your Google Cloud Console.
2. Download your service account key JSON file.
3. Set the environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="path/to/your/service-account-file.json"
export OPENAI_API_KEY="sk-..."
```

### â–¶ï¸ Usage

```bash
python main.py path/to/video.mp4
```

Output:

* Activity labels and transcript printed or saved
* Performance analysis generated via GPT-4o

---

## `demucs_whisper` Â ğŸ”¥ *(current fastest & most accurate)*

### ğŸš€ What It Does

| Stage                      | Tool / Settings                                                                                                                         |
| -------------------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| **Chunk video**            | `ffmpeg` â€“ 60â€¯s segments â†’ `/var/tmp/chunk_###.wav`                                                                                     |
| **Isolate teacher vocals** | **DemucsÂ `mdx_extra_q`** on **GPU**<br>Â Â â€¢Â `segmentâ€¯=â€¯15â€¯s`, `overlapâ€¯=â€¯0.25`, `shiftsâ€¯=â€¯0` (no 11Ã—â€¯TTA)<br>Â Â â€¢Â monoÂ â†’Â stereo duplication handled inâ€‘code |
| **Transcribe**             | **WhisperÂ largeâ€‘v3** on **GPU**, `fp16=True`, `language="en"`                                                                          |
| **Analyse**                | GPTâ€‘4o prompt for strengths / areas / summary                                                                               |

### ğŸ“ˆ ObservationsÂ (Whisperâ€‘onlyÂ vsÂ Demucsâ€¯+â€¯Whisper)

| Metric / Example                 | **Whisperâ€‘only**                                                | **Demucsâ€¯+â€¯Whisper**                                                       |
| -------------------------------- | ---------------------------------------------------------------- | -------------------------------------------------------------------------- |
| Speaker bleed                    | Childrenâ€™s hubbub dominates, teacher lines lost                 | Teacher channel isolated, backgroundâ€¯â‰«â€¯â€‘18â€¯dB                              |
| Sample lineÂ (ChunkÂ 2)            | `And stretch, zip your hipÂ wide â€¦` â†’ words blur in babble       | *â€œAnd stretchâ€¦ zip your hip wideâ€¦ bring your legs to the sideâ€¦â€*           |
| GPTâ€‘4o feedback                  | Generic (â€œstorytelling engagesâ€¦ improve clarityâ€)               | Specific, references *boat race*, *rollÂ the boat* activities               |


> **Key takeawayâ€¯ğŸŸ¢** â€“ with vocal separation the transcript becomes coherent enough that GPTâ€‘4o can produce actionable, lessonâ€‘specific feedback instead of boilerâ€‘plate.

## ğŸ“¦ Requirements

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install ffmpeg-python scipy diffq
pip install git+https://github.com/openai/whisper.git
pip install demucs==4.*
```
---

## â–¶ï¸ Usage

```bash
python main.py path/to/video.mp4
```

----

## âš ï¸ Note on Large Files

Do **not** commit `.mp4` files to the repo. Use a `.gitignore` to avoid tracking them:

```gitignore
*.mp4
```

---

## ğŸ“ File Structure

```
whisperaudio_chunk_transcribe_analyze.py   # Whisper-based pipeline
GCPVideoAI_GPT4o_Pipe/                     # Google Cloud Video AI + GPT-4o pipeline
README.md                                  # This file
<future files>                             # Other experiments (diarization, etc.)
```

---

More updates and experimental results coming soon.
